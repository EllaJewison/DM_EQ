
import requests
from bs4 import BeautifulSoup
# import pandas
# import lxml

URL = "https://www.allquakes.com/earthquakes/today.html"

page = requests.get(URL) # asking permission from the website to fetch data, If response is 200 its ok
# print(page.text)

soup = BeautifulSoup(page.content, "html.parser") # creating a bs object that takes page.content as an input
# (page.content is the HTML content I just scraped) sometime you need to change the second arg to another appropriate


# this gets all the html dat a from the "visible" table
table_EQ = soup.find('table', id="qTable")



# this finds all the earthquakes from magnitude 2 to magnitude 5
quakes = table_EQ.find_all(class_=lambda text: text in ['q2', 'q3', 'q4', 'q5'])



def find_quake_id():
    """ function that creates a list of quake ID. We will need it for getting the info on the second page
    Used for function in second page
    """
    quakes = table_EQ.find_all(class_=lambda text: text in ['q2', 'q3', 'q4', 'q5'])
    quake_id_list = []
    for q in quakes:
        id = q.get('id')
        quake_id_list.append(id)
    return quake_id_list



def find_url_in_table_EQ():
    """ This will find all the URL links in the table EQ and return a list of URL
    Used in function for second page
    """
    quakes = table_EQ.find_all(class_=lambda text: text in ['q2', 'q3', 'q4', 'q5'])
    url_list = []
    for q in quakes:
        info_link = (q.find_all("a"))[-1]
        link_url = info_link["href"]
        url_list.append(link_url)
    print(url_list)
    return url_list


def create_soup_from_link(link):
    """ creates soup from link """

    page = requests.get(URL)  # asking permission from the website to fetch data, If response is 200 its ok
    # print(page.text)
    soup = BeautifulSoup(page.content, "html.parser")  # creating a bs object that takes page.content as an input
    return soup


def builds_table_EQ():
    """ This function will build a table with all the earthquakes visible from the 1st page.
     It will return something non readable by humankind """

    table_EQ = soup.find('table', id="qTable")
    return table_EQ # at this stage its not readable for a human


def cleans_from_html_nonsense(table_EQ_dirty):
    """ This will take the table and make it readable-ish waiting until we can use pandas :) """

    cells = []
    for i in table_EQ_dirty.find_all('td'):
        title = i.text
        cells.append(title)

    headers = cells[0:6]
    table_EQ = []
    for i in range(7, len(cells), len(headers)):
        row = cells[i:i+6]
        print(row)
        table_EQ.append(row)

    return table_EQ


def scrap_from_p2(quake_id, URL):
    """ This function takes the quake id and the url link of an earthquake,  and opens the "more page" and
    hopefully retrieves the data  """
    URL = "https://www.volcanodiscovery.com/"+ URL
    page = requests.get(URL)
    soup = BeautifulSoup(page.content, "html.parser")
    table_p2 =[]
    table_rows = soup.find_all('tr')
    for tr in table_rows:
        td = tr.find_all('td')
        row = [i.text for i in td]
        print(row)
        table_p2.append(row)
    return table_p2


def main_scapper_p1():
    """ This will scrap the data from the table in the 1st page """

    url = "https://www.allquakes.com/earthquakes/today.html"
    soup = create_soup_from_link(url)
    table_EQ_dirty = builds_table_EQ()
    table_EQ = cleans_from_html_nonsense(table_EQ_dirty)
    print(table_EQ)
    url_list = find_url_in_table_EQ()
    quake_id_list = find_quake_id()

### this scrapes the second page !!!!!!!!!!

    tuple_id = zip(quake_id_list,url_list)
    for elem in tuple_id:
        # print(elem)
        table_p2 = scrap_from_p2(elem[0],elem[1])
        print('table_p2 : ', table_p2)
        print()



print(main_scapper_p1())








